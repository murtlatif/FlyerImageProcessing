{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "product_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1zqvvXpmEVK"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Deg83xp-mIHi"
      },
      "source": [
        "#https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# specify GPU\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "hmqMbsV3mMLe",
        "outputId": "49ac4052-1469-485b-acf1-49ebe2f3b59d"
      },
      "source": [
        "# load data\n",
        "df = pd.read_csv(\"data.csv\")\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>bakery</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>pudding</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>cake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>tart</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>candy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label     text\n",
              "0      3   bakery\n",
              "1      3  pudding\n",
              "2      3     cake\n",
              "3      3     tart\n",
              "4      3    candy"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6U2LtI4Nme_D"
      },
      "source": [
        "# split train dataset into train, validation and test sets\n",
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'], \n",
        "                                                                    random_state=2018, \n",
        "                                                                    test_size=0.3, \n",
        "                                                                    stratify=df['label'])\n",
        "\n",
        "\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
        "                                                                random_state=2018, \n",
        "                                                                test_size=0.5, \n",
        "                                                                stratify=temp_labels)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AFv3Qffmyw8",
        "outputId": "bec3b65e-b3dc-4eef-cafa-d9eae8cee948"
      },
      "source": [
        "# import BERT-base pretrained model\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqpI2bevnKkU",
        "outputId": "91577717-9484-4e61-f7a6-ef4e98568874"
      },
      "source": [
        "# sample data\n",
        "sample_text = [\"potato\", \"beef flank\"]\n",
        "\n",
        "# encode text\n",
        "sent_id = tokenizer.batch_encode_plus(sample_text, padding=True)\n",
        "\n",
        "# output\n",
        "print(sent_id)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[101, 14557, 102, 0], [101, 12486, 12205, 102]], 'token_type_ids': [[0, 0, 0, 0], [0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 0], [1, 1, 1, 1]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "uOLb3D-SnX8c",
        "outputId": "f5a3a34d-6769-435f-f5cb-72e5fe8a5b40"
      },
      "source": [
        "# get length of all the messages in the train set\n",
        "seq_len = [len(i.split()) for i in train_text]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)\n",
        "\n",
        "# padding will be 5 from the histogram"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f8d72265410>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_pRHYe-nibP",
        "outputId": "c55e8847-dc9b-4568-fec6-38e4e5fad84e"
      },
      "source": [
        "# tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = 5,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = 5,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = 5,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40ZNEId3nwcz"
      },
      "source": [
        "## convert lists to tensors\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nP4eC_Hn22x"
      },
      "source": [
        "#define a batch size\n",
        "batch_size = 32\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYjZygYhoB8a"
      },
      "source": [
        "# freeze all the parameters\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDOZgYizoEFF"
      },
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, bert):\n",
        "      super(BERT_Arch, self).__init__()\n",
        "      self.bert = bert \n",
        "      # dropout layer\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      # relu activation function\n",
        "      self.relu =  nn.ReLU()\n",
        "      # dense layer 1\n",
        "      self.fc1 = nn.Linear(768,512)\n",
        "      # dense layer 2 (Output layer)\n",
        "      self.fc2 = nn.Linear(512,5)\n",
        "      #softmax activation function\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "      #pass the inputs to the model  \n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
        "      x = self.fc1(cls_hs)\n",
        "      x = self.relu(x)\n",
        "      x = self.dropout(x)\n",
        "      # output layer\n",
        "      x = self.fc2(x)\n",
        "      # apply softmax activation\n",
        "      x = self.softmax(x)\n",
        "\n",
        "      return x"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Q2ZZsCmoPxc"
      },
      "source": [
        "# pass the pre-trained BERT to our define architecture\n",
        "model = BERT_Arch(bert)\n",
        "\n",
        "# push the model to GPU\n",
        "# model = model.to(device)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbP53QAPoR8k"
      },
      "source": [
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(),lr = 1e-5)          # learning rate"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVpevvLCoXEs",
        "outputId": "0888f394-c8ac-4f8b-848d-ad2c2f1a4f63"
      },
      "source": [
        "# for unbalanced datasets\n",
        "\n",
        "#compute the class weights\n",
        "class_weights = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
        "\n",
        "print(\"Class Weights:\",class_weights)\n",
        "\n",
        "# converting list of class weights to a tensor\n",
        "weights= torch.tensor(class_weights,dtype=torch.float)\n",
        "\n",
        "# # push to GPU\n",
        "# weights = weights.to(device)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Weights: [0.51826087 1.24166667 0.93125    3.50588235 1.1037037 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N8aut1-oc_I"
      },
      "source": [
        "# define the loss function\n",
        "cross_entropy  = nn.NLLLoss(weight=weights) \n",
        "\n",
        "# number of training epochs\n",
        "epochs = 10"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4xCsjy_okWi"
      },
      "source": [
        "# function to train the model\n",
        "def train():\n",
        "  model.train()\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "    # push the batch to gpu\n",
        "    # batch = [r.to(device) for r in batch]\n",
        "    sent_id, mask, labels = batch\n",
        "    # clear previously calculated gradients \n",
        "    model.zero_grad()        \n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JosqmiiRpP1T"
      },
      "source": [
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "  \n",
        "  print(\"\\nEvaluating...\")\n",
        "  \n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "    \n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      \n",
        "      # Calculate elapsed time in minutes.\n",
        "      elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    # batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyfgAn5gpTyQ"
      },
      "source": [
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "    \n",
        "    #evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHtfZuNwpX6K",
        "outputId": "746fe1b9-c64f-47d2-ea82-4705594c7af7"
      },
      "source": [
        "#load weights of best model\n",
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D05RqIfTrAsE"
      },
      "source": [
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  preds = model(test_seq, test_mask)\n",
        "  preds = preds.detach().cpu().numpy()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9D0hO1nrC4A",
        "outputId": "a569f3ce-e069-4368-89ec-f33e921675b0"
      },
      "source": [
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, preds))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.24      0.32        25\n",
            "           1       0.12      0.20      0.15        10\n",
            "           2       0.38      0.36      0.37        14\n",
            "           3       0.00      0.00      0.00         3\n",
            "           4       0.14      0.25      0.18        12\n",
            "\n",
            "    accuracy                           0.25        64\n",
            "   macro avg       0.23      0.21      0.20        64\n",
            "weighted avg       0.32      0.25      0.26        64\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCRjQ84NryY2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}